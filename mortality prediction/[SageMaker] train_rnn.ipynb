{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "from utils import utils\n",
    "from utils.readers import read_ts\n",
    "from models.preprocessing import Discretizer, Normalizer\n",
    "from models.create_normalizer_state import create_normalizer\n",
    "from models import metrics\n",
    "from models import keras_utils\n",
    "from models import common_utils\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "common_utils.add_common_arguments(parser)\n",
    "args = parser.parse_args()\n",
    "args.data='s3://aws-glue-scripts-271538242229-us-west-2/data/in-hospital-mortality/' # preprocessed data is imported from s3\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.network='models/rnn.py' \n",
    "args.period2=48.0\n",
    "args.time_step=1.0 \n",
    "#args.depth=1\n",
    "args.dim=16\n",
    "args.rec_dropout=0.3\n",
    "args.dropout=0.0\n",
    "args.mode='train' \n",
    "args.output_dir='models/output'\n",
    "'''  \n",
    "# uncomment following if test mode\n",
    "# model weights and predictions to output in args.output_dir\n",
    "'''\n",
    "# args.load_state='models/output/k_lstm.n2.d0.3.dep1.bs4096.ts1.0.epoch1.test0.6755971312522888.state'\n",
    "\n",
    "args.epochs=1\n",
    "args.batch_size=8\n",
    "\n",
    "if args.small_part:\n",
    "    args.save_every = 2**30\n",
    "\n",
    "args.data='data/in-hospital-mortality'\n",
    "train_reader = read_ts(dataset_dir='data/in-hospital-mortality/train/',\n",
    "                                         listfile='data/in-hospital-mortality/train_listfile.csv',                       \n",
    "                                         period_length=args.period2)\n",
    "val_reader = read_ts(dataset_dir='data/in-hospital-mortality/train/',\n",
    "                                         listfile='data/in-hospital-mortality/val_listfile.csv',                           \n",
    "                                         period_length=args.period2)\n",
    "\n",
    "'''\n",
    "uncomment following if run locally    \n",
    "'''\n",
    "# args.data='C:/Users/sy/Downloads/cse 6250/project/mimic3/data/in-hospital-mortality'\n",
    "# train_reader = read_ts(dataset_dir=os.path.join(args.data, 'train'),\n",
    "#                                      listfile=os.path.join(args.data, 'train_listfile.csv'),\n",
    "#                                      period_length=args.period2)\n",
    "# val_reader = read_ts(dataset_dir=os.path.join(args.data, 'train'),\n",
    "#                                    listfile=os.path.join(args.data, 'val_listfile.csv'),\n",
    "#                                    period_length=args.period2)\n",
    "\n",
    "#create discretizer\n",
    "discretizer = Discretizer(timestep=float(args.timestep),\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')\n",
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "\n",
    "args_dict = dict(args._get_kwargs())\n",
    "args.target_repl_coef=0.5\n",
    "target_repl = (args.target_repl_coef > 0.0 and args.mode == 'train')\n",
    "args_dict['target_repl'] = target_repl\n",
    "args_dict['model']='GRU' \n",
    "args_dict['is_bidirectional']=True \n",
    "args_dict['depth']=2\n",
    "args_dict['channel_wise']=True \n",
    "args_dict['header'] = discretizer_header # for cw\n",
    "\n",
    "if args.small_part:\n",
    "    args.save_every = 2**30\n",
    "\n",
    "'''\n",
    "uncomment following if you want to use different normalizer to train new model \n",
    "'''\n",
    "# '--timestep' : Rate of the re-sampling to discretize time-series. default=1.0\n",
    "#'--impute_strategy': 'Strategy for imputing missing values.' choices=['zero', 'next', 'previous', 'normal_value'],\n",
    "#'--start_time': start time of discretization. choices=['zero', 'relative'] Zero means beginning of the ICU stay. Relative use the time of the first ICU event'  \n",
    "#'--n_samples': 'How many samples to use to estimates means and standard deviations. Set -1 to use all training samples.\n",
    "#'--output_dir':default='models'\n",
    "#'--data' : 'Path to the data.'\n",
    "\n",
    "# create_normalizer(timestep=1.0, impute_strategy='previous',start_time='zero', n_samples=10, dataset_dir='data/in-hospital-mortality/train/',\n",
    "#                                          listfile='data/in-hospital-mortality/train_listfile.csv', output_dir='models')\n",
    "\n",
    "#load normalizer\n",
    "\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "normalizer = Normalizer(fields=cont_channels)   # choose which  columns to standardize\n",
    "args.normalizer_state='ts1.00_imputeprevious_startzero_masksTrue_n17903.normalizer' # create new normalizer in the block above as  needed \n",
    "normalizer_state = args.normalizer_state\n",
    "if normalizer_state is None:    \n",
    "    create_normalizer(data=args.data)\n",
    "normalizer_state= os.path.join('models/',normalizer_state)            \n",
    "normalizer.load_params(normalizer_state)\n",
    "\n",
    "# build the model\n",
    "print(\"==> using model {}\".format(args.network))\n",
    "model_module = imp.load_source(os.path.basename(args.network), args.network)\n",
    "\n",
    "model = model_module.Network(**args_dict)\n",
    "\n",
    "suffix = \".bs{}{}{}.ts{}{}\".format(args.batch_size,\n",
    "                                   \".L1{}\".format(args.l1) if args.l1 > 0 else \"\",\n",
    "                                   \".L2{}\".format(args.l2) if args.l2 > 0 else \"\",\n",
    "                                   args.timestep,\n",
    "                                   \".trc{}\".format(args.target_repl_coef) if args.target_repl_coef > 0 else \"\")\n",
    "model.final_name = model.say_name() + suffix\n",
    "print(\"==> model.final_name:\", model.final_name)\n",
    "\n",
    "print(\"==> compiling the model\")\n",
    "optimizer_config = {'class_name': args.optimizer,\n",
    "                    'config': {'lr': args.lr,\n",
    "                               'beta_1': args.beta_1}}\n",
    "\n",
    "if target_repl:\n",
    "    loss = ['binary_crossentropy'] * 2\n",
    "    loss_weights = [1 - args.target_repl_coef, args.target_repl_coef]\n",
    "else:\n",
    "    loss = 'binary_crossentropy'\n",
    "    loss_weights = None\n",
    "\n",
    "model.compile(optimizer=optimizer_config,\n",
    "              loss=loss,\n",
    "              loss_weights=loss_weights)\n",
    "model.summary()\n",
    "\n",
    "# Load model weights\n",
    "n_trained_chunks = 0\n",
    "#print (args.load_state)\n",
    "\n",
    "if args.load_state != None:\n",
    "    model.load_weights(args.load_state)\n",
    "    n_trained_chunks = int(re.match(\".*epoch([0-9]+).*\", args.load_state).group(1))\n",
    "\n",
    "\n",
    "# read data\n",
    "train_raw = utils.load_data(train_reader, discretizer, normalizer, args.small_part)\n",
    "val_raw = utils.load_data(val_reader, discretizer, normalizer, args.small_part)\n",
    "\n",
    "if target_repl:\n",
    "    T = train_raw[0][0].shape[0]\n",
    "\n",
    "    def extend_labels(data):\n",
    "        data = list(data)\n",
    "        labels = np.array(data[1])  # (B,)\n",
    "        data[1] = [labels, None]\n",
    "        data[1][1] = np.expand_dims(labels, axis=-1).repeat(T, axis=1)  # (B, T)\n",
    "        data[1][1] = np.expand_dims(data[1][1], axis=-1)  # (B, T, 1)\n",
    "        return data\n",
    "\n",
    "    train_raw = extend_labels(train_raw)\n",
    "    val_raw = extend_labels(val_raw)\n",
    "\n",
    "if args.mode == 'train':\n",
    "\n",
    "    path = os.path.join(args.output_dir, model.final_name + '.epoch{epoch}.test{val_loss}.state')\n",
    "\n",
    "    metrics_callback = keras_utils.keras_metrics(train_data=train_raw, val_data=val_raw,\n",
    "                                                              target_repl=(args.target_repl_coef > 0),\n",
    "                                                              batch_size=args.batch_size,\n",
    "                                                              verbose=args.verbose)\n",
    "    dirname = os.path.dirname(path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    saver = ModelCheckpoint(path, verbose=1, period=args.save_every)\n",
    "\n",
    "    logs = os.path.join(args.output_dir, 'logs')\n",
    "    if not os.path.exists(logs):\n",
    "        os.makedirs(logs)\n",
    "    csv_logger = CSVLogger(os.path.join(logs, model.final_name + '.csv'),\n",
    "                           append=True, separator=';')\n",
    "\n",
    "    print(\"==> training\")\n",
    "    model.fit(x=train_raw[0],\n",
    "              y=train_raw[1],\n",
    "              validation_data=val_raw,\n",
    "              epochs=n_trained_chunks + args.epochs,\n",
    "              initial_epoch=n_trained_chunks,\n",
    "              callbacks=[metrics_callback, saver, csv_logger],\n",
    "              shuffle=True,\n",
    "              verbose=args.verbose,\n",
    "              batch_size=args.batch_size)\n",
    "\n",
    "elif args.mode == 'test':\n",
    "    del train_reader\n",
    "    del val_reader\n",
    "    del train_raw\n",
    "    del val_raw\n",
    "\n",
    "    test_reader = read_ts(dataset_dir=os.path.join(args.data, 'test'),\n",
    "                                            listfile=os.path.join(args.data, 'test_listfile.csv'),\n",
    "                                            period_length=args.period2)\n",
    "    ret = utils.load_data(test_reader, discretizer, normalizer, args.small_part,\n",
    "                          return_names=True)\n",
    "\n",
    "    data = ret[\"data\"][0]\n",
    "    labels = ret[\"data\"][1]\n",
    "    names = ret[\"names\"]\n",
    "\n",
    "    predictions = model.predict(data, batch_size=args.batch_size, verbose=1)\n",
    "    predictions = np.array(predictions)[:, 0]\n",
    "    metrics.print_metrics_binary(labels, predictions)\n",
    "\n",
    "    path = os.path.join(args.output_dir, \"test_predictions\", os.path.basename(args.load_state)) + \".csv\"\n",
    "    utils.save_results(names, predictions, labels, path)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Wrong value for args.mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
